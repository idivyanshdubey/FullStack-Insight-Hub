{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffca218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rank, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a6164d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameExample\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5392edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ðŸŸ¢ BASIC EXAMPLES\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181b2598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Create a DataFrame from a Python list\n",
    "# Each tuple is a row, and we define column names\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a99f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------------+\n",
      "|Code|Symbol|                Name|\n",
      "+----+------+--------------------+\n",
      "| AED|   Ø¯.Ø¥|United Arab Emira...|\n",
      "| AFN|     Ø‹|      Afghan afghani|\n",
      "| ALL|     L|        Albanian lek|\n",
      "| AMD|   AMD|       Armenian dram|\n",
      "| ANG|     Æ’|Netherlands Antil...|\n",
      "| AOA|    Kz|      Angolan kwanza|\n",
      "| ARS|     $|      Argentine peso|\n",
      "| AUD|     $|   Australian dollar|\n",
      "| AWG|  Afl.|       Aruban florin|\n",
      "| AZN|   AZN|   Azerbaijani manat|\n",
      "| BAM|    KM|Bosnia and Herzeg...|\n",
      "| BBD|     $|    Barbadian dollar|\n",
      "| BDT|    à§³ |    Bangladeshi taka|\n",
      "| BGN|   Ð»Ð².|       Bulgarian lev|\n",
      "| BHD|  .Ø¯.Ø¨|      Bahraini dinar|\n",
      "| BIF|    Fr|     Burundian franc|\n",
      "| BMD|     $|    Bermudian dollar|\n",
      "| BND|     $|       Brunei dollar|\n",
      "| BOB|   Bs.|  Bolivian boliviano|\n",
      "| BRL|    R$|      Brazilian real|\n",
      "+----+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Read a CSV file into a DataFrame\n",
    "# Assumes 'data.csv' exists with headers\n",
    "df_csv = spark.read.csv(\"currency.csv\", header=True, inferSchema=True)\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49152f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   Name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "|Charlie|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Select specific columns and filter rows\n",
    "# Select only the 'Name' column\n",
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee60c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where Age is greater than 30\n",
    "df.filter(df.Age > 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155c3112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|AgePlusTen|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|        35|\n",
      "|    Bob| 30|        40|\n",
      "|Charlie| 35|        45|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Add a new column by transforming existing data\n",
    "# Adds 10 to each person's age\n",
    "df = df.withColumn(\"AgePlusTen\", col(\"Age\") + 10)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac3c",
   "metadata": {},
   "source": [
    "# -------------------------------\n",
    "# ðŸŸ¡ INTERMEDIATE EXAMPLES\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a41f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|avg(Salary)|\n",
      "+----------+-----------+\n",
      "|        HR|     5250.0|\n",
      "|        IT|     6000.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Group by a column and perform aggregation\n",
    "# Calculates average salary per department\n",
    "data_group = [(\"Alice\", \"HR\", 5000), (\"Bob\", \"IT\", 6000), (\"Charlie\", \"HR\", 5500)]\n",
    "df_group = spark.createDataFrame(data_group, [\"Name\", \"Department\", \"Salary\"])\n",
    "df_group.groupBy(\"Department\").agg({\"Salary\": \"avg\"}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d38f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|Dept| Name|Salary|\n",
      "+----+-----+------+\n",
      "|  HR|Alice|  5000|\n",
      "|  IT|  Bob|  6000|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Join two DataFrames on a common column\n",
    "# Joins employee names with department salaries\n",
    "data1 = [(\"Alice\", \"HR\"), (\"Bob\", \"IT\")]\n",
    "data2 = [(\"HR\", 5000), (\"IT\", 6000)]\n",
    "df1 = spark.createDataFrame(data1, [\"Name\", \"Dept\"])\n",
    "df2 = spark.createDataFrame(data2, [\"Dept\", \"Salary\"])\n",
    "joined = df1.join(df2, on=\"Dept\", how=\"inner\")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcd601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Run SQL queries on a DataFrame\n",
    "# Creates a temporary SQL view and queries it\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT Name, Age FROM people WHERE Age > 30\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd591153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice|  0|\n",
      "|  Bob| 30|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Handle missing data\n",
    "# Fill missing Age values with 0\n",
    "df_missing = spark.createDataFrame([(\"Alice\", None), (\"Bob\", 30)], [\"Name\", \"Age\"])\n",
    "df_missing.na.fill({\"Age\": 0}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f586688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "| Bob| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any missing values\n",
    "df_missing.na.drop().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e35c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ðŸ”µ MODERATE EXAMPLES\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9fbb209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+----+\n",
      "|   Name|Dept|Salary|Rank|\n",
      "+-------+----+------+----+\n",
      "|    Bob|  HR|  5500|   1|\n",
      "|  Alice|  HR|  5000|   2|\n",
      "|Charlie|  IT|  6000|   1|\n",
      "+-------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Use window functions for ranking\n",
    "# Ranks employees by salary within each department\n",
    "data_window = [(\"Alice\", \"HR\", 5000), (\"Bob\", \"HR\", 5500), (\"Charlie\", \"IT\", 6000)]\n",
    "df_window = spark.createDataFrame(data_window, [\"Name\", \"Dept\", \"Salary\"])\n",
    "windowSpec = Window.partitionBy(\"Dept\").orderBy(col(\"Salary\").desc())\n",
    "df_window.withColumn(\"Rank\", rank().over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d56be144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+--------+\n",
      "|   Name|Age|AgePlusTen|Category|\n",
      "+-------+---+----------+--------+\n",
      "|  Alice| 25|        35|   Young|\n",
      "|    Bob| 30|        40|     Old|\n",
      "|Charlie| 35|        45|     Old|\n",
      "+-------+---+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Create and use a User Defined Function (UDF)\n",
    "# Categorizes people as 'Young' or 'Old' based on age\n",
    "def age_category(age):\n",
    "    return \"Young\" if age < 30 else \"Old\"\n",
    "\n",
    "age_udf = udf(age_category, StringType())\n",
    "df = df.withColumn(\"Category\", age_udf(col(\"Age\")))\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
