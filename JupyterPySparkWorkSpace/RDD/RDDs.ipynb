{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d41bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rank, udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b0fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57127)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\PySparkTools\\WinPython311\\WPy64-31140\\python-3.11.4.amd64\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"c:\\PySparkTools\\WinPython311\\WPy64-31140\\python-3.11.4.amd64\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"c:\\PySparkTools\\WinPython311\\WPy64-31140\\python-3.11.4.amd64\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"c:\\PySparkTools\\WinPython311\\WPy64-31140\\python-3.11.4.amd64\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\Program Files\\Spark-3.5.5\\python\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\Program Files\\Spark-3.5.5\\python\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"C:\\Program Files\\Spark-3.5.5\\python\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\Spark-3.5.5\\python\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"c:\\PySparkTools\\WinPython311\\WPy64-31140\\python-3.11.4.amd64\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDDsExample\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b598befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ðŸŸ¢ BASIC RDD EXAMPLES\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478e431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Contents: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create an RDD from a Python list\n",
    "# Parallelize converts a local list into a distributed RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(\"RDD Contents:\", rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24d6354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped RDD: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "# 2. Map transformation\n",
    "# Multiply each element by 2\n",
    "rdd_map = rdd.map(lambda x: x * 2)\n",
    "print(\"Mapped RDD:\", rdd_map.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917d19f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered RDD: [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# 3. Filter transformation\n",
    "# Keep only even numbers\n",
    "rdd_filter = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Filtered RDD:\", rdd_filter.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bb98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of RDD: 15\n"
     ]
    }
   ],
   "source": [
    "# 4. Reduce action\n",
    "# Sum all elements\n",
    "rdd_sum = rdd.reduce(lambda x, y: x + y)\n",
    "print(\"Sum of RDD:\", rdd_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a4ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ðŸŸ¡ INTERMEDIATE RDD EXAMPLES\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dcdfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['hello', 'world', 'how', 'are', 'you']\n"
     ]
    }
   ],
   "source": [
    "# 5. FlatMap transformation\n",
    "# Split sentences into words\n",
    "sentences = [\"hello world\", \"how are you\"]\n",
    "rdd_text = spark.sparkContext.parallelize(sentences)\n",
    "rdd_words = rdd_text.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"Words:\", rdd_words.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b532123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced by Key: [('a', 4), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 6. Key-Value Pair RDD and reduceByKey\n",
    "# Aggregate values by key\n",
    "pairs = [(\"a\", 1), (\"b\", 2), (\"a\", 3)]\n",
    "rdd_kv = spark.sparkContext.parallelize(pairs)\n",
    "rdd_reduced = rdd_kv.reduceByKey(lambda x, y: x + y)\n",
    "print(\"Reduced by Key:\", rdd_reduced.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5528818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped by Key: [('a', [1, 3]), ('b', [2])]\n"
     ]
    }
   ],
   "source": [
    "# 7. GroupByKey\n",
    "# Groups values under each key\n",
    "rdd_grouped = rdd_kv.groupByKey().mapValues(list)\n",
    "print(\"Grouped by Key:\", rdd_grouped.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# ðŸ”µ MODERATE RDD EXAMPLES\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf659952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined RDD: [('a', (1, 'apple')), ('b', (2, 'banana'))]\n"
     ]
    }
   ],
   "source": [
    "# 8. Join two Pair RDDs\n",
    "# Joins based on keys\n",
    "rdd1 = spark.sparkContext.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "rdd2 = spark.sparkContext.parallelize([(\"a\", \"apple\"), (\"b\", \"banana\")])\n",
    "rdd_joined = rdd1.join(rdd2)\n",
    "print(\"Joined RDD:\", rdd_joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7e6644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted RDD: [('c', 1), ('b', 2), ('a', 3)]\n"
     ]
    }
   ],
   "source": [
    "# 9. SortBy transformation\n",
    "# Sort elements by a custom key\n",
    "rdd_unsorted = spark.sparkContext.parallelize([(\"b\", 2), (\"a\", 3), (\"c\", 1)])\n",
    "rdd_sorted = rdd_unsorted.sortBy(lambda x: x[1])\n",
    "print(\"Sorted RDD:\", rdd_sorted.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d47eb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Save RDD to text file (optional)\n",
    "# Uncomment to save output to disk\n",
    "rdd_sorted.saveAsTextFile(\"output_rdd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
